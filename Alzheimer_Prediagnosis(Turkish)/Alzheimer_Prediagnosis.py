# -*- coding: utf-8 -*-
"""ALZHEIMER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FxWO8DP7ZDdHYKd9JtzcFPI6RZgIwJTf
"""

from google.colab import files #CSV DOSYASINI SEÇMEK İÇİN BU ŞEKİLDE YAPTIM.
uploaded = files.upload()

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import io #CSV DOSYASINI OKUMAK İÇİN KÜTÜPHANEYİ EKLEDİM
import numpy as np
import seaborn as sns #VERİ GÖRSELLEŞTİRMESİ İÇİN KULLANIYORUZ
import matplotlib.pyplot as plt
# %matplotlib inline

sns.set() # PARAGRAFLARI RESETLE

df = pd.read_csv(io.BytesIO(uploaded['oasis_longitudinal.csv'])) #BU KISMI DEĞİŞTİRDİM (cross_sectional YAZARSAK O DOSYAYI SEÇECEKTİR)
df.head()

df = df.loc[df['Visit']==1]
df = df.reset_index(drop=True)      #df.head() ile kontrol edebiliriz.Visit=1 OLANLARA YENİDEN SIRA NUMARASI VERDİK.

df['M/F'] = df['M/F'].replace(['F','M'], [0,1]) # FEMALE : 0 , MALE : 1 ATAMASI YAPTIK DAHA RAHAT İŞLEM YAPABİLMEK İÇİN
df.head()
df['Group'] = df['Group'].replace(['Converted'], ['Demented']) #CONVERTED YERİNE DEMENTED YAZDIK
df['Group'] = df['Group'].replace(['Demented', 'Nondemented'], [1,0])

df = df.drop(['MRI ID', 'Visit', 'Hand'], axis=1) #GEREKSİZ SÜTUNLARI SİLEBİLİRİZ KULLANMAYACAĞIZ BURDA.

# BAR ŞEKLİNDE ÇİZİM FONKSİYPONU
def bar_chart(feature):
    Demented = df[df['Group']==1][feature].value_counts() # value_counts İLE EN ÇOK TEKRAR EDENLERİ BULDUK.(MALE İÇİN)
    Nondemented = df[df['Group']==0][feature].value_counts() # value_counts İLE EN ÇOK TEKRAR EDENLERİ BULDUK(FEMALE İÇİN)
    df_bar = pd.DataFrame([Demented,Nondemented]) #VERİ ÇERÇEVESİ HALİNE GETİRMEK YANİ GÖRSELLEŞTİRMEK İÇİN DATAFRAME KULLANIYORUZ
    df_bar.index = ['Demented','Nondemented']
    df_bar.plot(kind='bar',stacked=False, figsize=(8,5))

# CİNSİYETLER VE GRUPLAR ŞEKLİNDE YAPIYORUZ (FEMALE : 0 , MALE :1 ŞEKLİNDE UYGUNLANDI)
bar_chart('M/F') # bar_chart FONKSİYONUNA BU ŞEKİLDE MALE VE FEMALE BİLGİSİNİ GÖNDERDİK
plt.xlabel('GRUP')
plt.ylabel('HASTA SAYISI')
plt.legend()
plt.title('Gender and Demented rate')

#MMSE : Mini Mental State Examination BU ALZHEIMER TESTİDİR SINIR 24 DÜR. 24 ÜSTÜ NORMAL ; ALTI NORMAL DEĞİLDİR. BU TESTTE KİŞİYE BAZI GÜNLÜK HAYATTAN SORULAR SORULARAK CEVAPLAR ALINIR VE ONA GÖRE PUANLAMA YAPILIR.
# Nondemented = 0, Demented =1
# NONDEMENTED 25-30 ARASI YÜKSEK TEST SONUÇLARINA SAHİP GÖRÜNDÜĞÜ GİBİ
#Min 17 ,MAX 30
facet= sns.FacetGrid(df,hue="Group", aspect=3) #aspect İLE GRAFİK ENİ BELİRLENİYOR
facet.map(sns.kdeplot,'MMSE',shade= True) #SHADE GRAFİK ALTINDA KALAN ALANLARI BOYAR
facet.set(xlim=(0, df['MMSE'].max()))
facet.add_legend()
plt.xlim(15.30)

#bar_chart('ASF') = Atlas Scaling Factor   HACİM ÖLÇEKLENDİRME FAKTÖRÜDÜR. MORFOMETRİK VE FONKSİYONEL ANALİZLER İÇİN KULLANILIR.
facet= sns.FacetGrid(df,hue="Group", aspect=3)
facet.map(sns.kdeplot,'ASF',shade= True)
facet.set(xlim=(0, df['ASF'].max()))
facet.add_legend()
plt.xlim(0.5, 2)

#eTIV = Estimated Total Intracranial Volume KAFA İÇİ BASINÇ ARTIŞINI TEMSİL EDİYOR. BASINÇ ARTIŞI BEYNİ SIKIŞTIRIR VE UNUTKANLIĞA SEBEP OLUR.ERKEKLER KADINLARDAN YAKLAŞIK % 12 DAHA BÜYÜK BİR eTIV GÖSTERMİŞTİR.
facet= sns.FacetGrid(df,hue="Group", aspect=3)
facet.map(sns.kdeplot,'eTIV',shade= True)
facet.set(xlim=(0, df['eTIV'].max()))
facet.add_legend()
plt.xlim(900, 2100)

#'nWBV' = Normalized Whole Brain Volume BEYİN HACMİNE DAYALI OTOMATİK DOKU SEGMENTASYONU TAHMİNİ (GRİ +BEYAZ MADDE).
# Nondemented = 0, Demented =1
facet= sns.FacetGrid(df,hue="Group", aspect=3)
facet.map(sns.kdeplot,'nWBV',shade= True)
facet.set(xlim=(0, df['nWBV'].max()))
facet.add_legend()
plt.xlim(0.6,0.9)

#YAŞ. Nondemented =0, Demented =1
facet= sns.FacetGrid(df,hue="Group", aspect=3)
facet.map(sns.kdeplot,'Age',shade= True)
facet.set(xlim=(0, df['Age'].max()))
facet.add_legend()
plt.xlim(50,100)

#'EDUC' = EĞİTİM ALDIĞI YIL SAYISI
# Nondemented = 0, Demented =1
facet= sns.FacetGrid(df,hue="Group", aspect=3)
facet.map(sns.kdeplot,'EDUC',shade= True)
facet.set(xlim=(df['EDUC'].min(), df['EDUC'].max()))
facet.add_legend()
plt.ylim(0, 0.16)

# HER BİR SÜTUNDA EKSİK VERİ KONTROLÜ YAPIYORUZ
pd.isnull(df).sum()
# SÜTUN, SES' de 8 KAYIP VERİMİZ VAR.( SES: Social Economic Status(SOSYOEKONOMİK DURUMU))

# SES'deki 8 EKSİK VERİ DROPOUT EDİLDİ.
df_dropna = df.dropna(axis=0, how='any')
pd.isnull(df_dropna).sum() # TEKRAR BOŞ VERİ VARMI KONTROL EDELİM !

df_dropna['Group'].value_counts() # DROPOUTTAN SONRA ERKEK VE KADIN SAYISINI YENİDEN BAKALIM! FEMALE : 0 , MALE : 1

# EDUC ve SES ARASI DAĞILIM GRAFİĞİ
x = df['EDUC']
y = df['SES']

ses_not_null_index = y[~y.isnull()].index
x = x[ses_not_null_index]
y = y[ses_not_null_index]

# TREND ÇİZGİSİ KIRMIZI ÇİZİLDİ
z = np.polyfit(x, y, 1)
p = np.poly1d(z)
plt.plot(x, y,'go', x, p(x), "r--")
plt.xlabel('Education Level(EDUC)')
plt.ylabel('Social Economic Status(SES)')

plt.show()

df.groupby(['EDUC'])['SES'].median() #ORTADAKİ SAYIYA BAKILIYOR

df["SES"].fillna(df.groupby("EDUC")["SES"].transform("median"), inplace=True) # ÜSTTEKİ ATAMAYA GÖRE LİSTEYE DE EKLEME YAPILDI.
df # HEPSİNİ YAZDIRIP KONTROL AMAÇLI GÖRELİM.

from sklearn.model_selection import train_test_split # TRAIN ve TEST KÜMELERİNİ BELİRLEMEK İÇİN BUNU KULLANIYORUZ. YANİ VERİ KÜMESİNİ İKİYE BÖLMEK İÇİN.percentage_split WEKA DA Kİ ADI :)
from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler  #VERİ ÜZERİNDE ÖLÇEKLENDİRME,EKSİK VERİ GİDERME İÇİN VS preprocessng KULLANIRIZ.MinMaxScaler 0-1 ARASINDA NORMALİZASYON YAPMAMIZI SAĞLAR.
from sklearn.model_selection import cross_val_score #BAŞKA BİR TEST MODU DA cross_validation DUR.
from sklearn.decomposition import PCA # PCA İŞLEMİNİ GERÇEKLEŞTİRMEK İÇİN KULLANACAĞIZ

# İMPUTASYON YAPILMIŞ ŞEKİLDE VERİ SIRALANDI (KAYIP DEĞERLERİ DÜZENLEME)(ZATEN VERİLERİMİZDE EKSİK YOK YUKARIDA BU İŞİ YAPTIK.)
Y = df['Group'].values
X = df[['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF']] # KULLANACAĞIMIZ ÖZNİTELİKLER

# 3 GRUBA AYRILDI.
X_trainval, X_test, Y_trainval, Y_test = train_test_split(
    X, Y, random_state=0)

# NORMALİZASYON
scaler = MinMaxScaler().fit(X_trainval)
X_trainval_scaled = scaler.transform(X_trainval) # ÖLÇEKLENDİRME
X_test_scaled = scaler.transform(X_test)

# PCA UYGULAMA (BURAYI KAPATIP MODELİMİZİ EĞİTECEĞİZ,DAHA SONRA AÇIP TEKRAR DENEYECEĞİZ)
"""pca = PCA(n_components = 5)
X_trainval_scaled = pca.fit_transform(X_trainval_scaled)
X_test_scaled = pca.transform(X_test_scaled)"""

# EKSİK DEĞERLERİ DROP ETTİKTEN SONRA VERİ SETİMİZ
Y = df_dropna['Group'].values
X = df_dropna[['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF']]

# 3 GRUBA DAĞITILDI.
X_trainval_dna, X_test_dna, Y_trainval_dna, Y_test_dna = train_test_split(
    X, Y, random_state=0)

# NORMALİZASYON
scaler = MinMaxScaler().fit(X_trainval_dna)
X_trainval_scaled_dna = scaler.transform(X_trainval_dna)
X_test_scaled_dna = scaler.transform(X_test_dna)

# PCA UYGULAMA (BURAYI KAPATIP MODELİMİZİ EĞİTECEĞİZ,DAHA SONRA AÇIP TEKRAR DENEYECEĞİZ) PCA İYİ BİR DEĞER VERMEDİĞİ İÇİN UYGULAMIYORUZ
pca = PCA(n_components = 5)
X_trainval_scaled_dna = pca.fit_transform(X_trainval_scaled_dna)
X_test_scaled_dna = pca.transform(X_test_scaled_dna)

# İMPUTASYON YAPILMIŞ ŞEKİLDE VERİ SIRALANDI (KAYIP DEĞERLERİ DÜZENLEME)
Y = df['Group'].values #BU KOD BLOĞU SVM'YE ÖZEL YAPILDI
X = df[['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF']] # KULLANACAĞIMIZ ÖZELLİKLER(ÖZNİTELİKLER)

# 3 GRUBA AYRILDI.
X_trainval1, X_test1, Y_trainval1, Y_test1 = train_test_split(
    X, Y, random_state=0)

# NORMALİZASYON
scaler = MinMaxScaler().fit(X_trainval1)
X_trainval_scaled1 = scaler.transform(X_trainval1) # ÖLÇEKLENDİRME
X_test_scaled1 = scaler.transform(X_test1)

from sklearn.linear_model import LogisticRegression # KULLANCAĞIMIZ SINIFLANDIRMA ALGORİTMALARI VE SINIFLANDIRMA SONUNDA NE KADAR DOĞRU SINIFLANDIRMA YAPTIĞINA DAİR METRİKLER ALINDI.
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, roc_curve, auc, precision_score, f1_score

acc = [] # BÜTÜN PERFORMANS METRİKLERİNİ BURDA DEPOLAYACAĞIZ. LİSTE YAPISI OLUŞTURDUK İŞLEMLERİ YAPTIKÇA DOLDURACAĞIZ.

# İMPUTASYON YAPILARAK VERİSETİ (İMPUTASYON. EKSİK DEĞERLERİ DÜZENLEYEREK.DAHA SONRA EKSİK VERİLERİ SİLİNMİŞ DATASET ÜZERİNDE ÇALIŞACAĞIZ.)
best_score = 0 # PCA UYGULAYINCA DAHA YÜKSEK DOĞRULUK DEĞERİ ELDE EDİLDİ!!
kfolds = 10  # k fold GİRİŞİ . GENELDE 10 SEÇİLİR BUNU YENİDEN ARAŞTIR. cross_validation UYGULANIYOR.

for c in [0.001, 0.1, 1, 10, 100]:
    logRegModel = LogisticRegression(C=c)  # LOJİSTİK REGRESYON MODELİ OLUŞTURUYORUZ
    # cross_validation İŞLEMİ GERÇEKLEŞTİRİYORUZ
    scores = cross_val_score(logRegModel, X_trainval, Y_trainval, cv=kfolds,
                             scoring='accuracy')

    # cross_validation SONUCU ELDE ETTİĞİMİZ ACCURACY DEĞERLERİNİN ORTALAMASINI ALIYORUZ.
    score = np.mean(scores)

    # EN İYİ score DEĞERİNİ BULMAYA ÇALIŞIYORUZ
    if score > best_score:
        best_score = score
        best_parameters = c

# TRAINING VE TEST SETLERİNİ KULLANARAK MODELİMİZ GÜNCELLİYORUZ.
SelectedLogRegModel = LogisticRegression(C=best_parameters).fit(X_trainval_scaled, Y_trainval)

test_score = SelectedLogRegModel.score(X_test_scaled, Y_test)
PredictedOutput = SelectedLogRegModel.predict(X_test_scaled)
test_recall = recall_score(Y_test, PredictedOutput, pos_label=1)
fpr, tpr, thresholds = roc_curve(Y_test, PredictedOutput, pos_label=1)
test_auc = auc(fpr, tpr)
test_precision = precision_score(Y_test, PredictedOutput, pos_label=1)
f1 = f1_score(Y_test, PredictedOutput, pos_label=1)

print("VALİDASYON SETİ İÇİN EN İYİ ACCURACY DEĞERİ : ", best_score)
print("REGULERİZASYON İÇİN EN UYGUN PARAMETRE DEĞERİ (c) : ", best_parameters)
print("TEST ACCURACY DEĞERİ(EN İYİ PARAMETRE İÇİN)", test_score)
print("TEST RECALL DEĞERİ (EN İYİ PARAMETRE İÇİN)", test_recall)
print("TEST AUC DEĞERİ (EN İYİ PARAMETRE DEĞERİ İÇİN)", test_auc)
print("TEST PRECISION DEĞERİ(EN İYİ PARAMETRE İÇİN)", test_precision)
print("F1-SCORE SONUCU", f1)

m = 'Logistic Regression UYGULAMASI(İMPUTASYON YAPILAN)'
acc.append([m, test_score, test_recall, test_auc, test_precision,f1,fpr, tpr, thresholds])

# ŞİMDİ İSE EKSİK VERİLERİ DROP EDEREK YENİDEN AYNI MODELİ OLUŞTURALIM.BAKALIM HANGİSİNDE DAHA İYİ SONUÇ ALACAĞIZ
best_score=0
kfolds=10

for c in [0.001, 0.1, 1, 10, 100]:
    logRegModel = LogisticRegression(C=c)
    # cross_validation İŞLEMİ GERÇEKLEŞTİRİYORUZ
    scores = cross_val_score(logRegModel, X_trainval_scaled_dna, Y_trainval_dna, cv=kfolds, scoring='accuracy')

    # cross_validation SONUCU ELDE ETTİĞİMİZ ACCURACY DEĞERLERİNİN ORTALAMASINI ALIYORUZ.
    score = np.mean(scores)

    # EN İYİ score DEĞERİNİ BULMAYA ÇALIŞIYORUZ
    if score > best_score:
        best_score = score
        best_parameters = c

# TRAINING VE TEST SETLERİNİ KULLANARAK MODELİMİZ GÜNCELLİYORUZ.
SelectedLogRegModel = LogisticRegression(C=best_parameters).fit(X_trainval_scaled_dna, Y_trainval_dna)

test_score = SelectedLogRegModel.score(X_test_scaled_dna, Y_test_dna)
PredictedOutput = SelectedLogRegModel.predict(X_test_scaled_dna)
test_recall = recall_score(Y_test_dna, PredictedOutput, pos_label=1)
fpr, tpr, thresholds = roc_curve(Y_test_dna, PredictedOutput, pos_label=1)
test_auc = auc(fpr, tpr)
print("VALİDASYON SETİ İÇİN EN İYİ ACCURACY DEĞERİ : ", best_score)
print("REGULERİZASYON İÇİN EN UYGUN PARAMETRE DEĞERİ (c) : ", best_parameters)
print("TEST ACCURACY DEĞERİ(EN İYİ PARAMETRE İÇİN)", test_score)
print("TEST RECALL DEĞERİ (EN İYİ PARAMETRE İÇİN)", test_recall)
print("TEST AUC DEĞERİ (EN İYİ PARAMETRE DEĞERİ İÇİN)", test_auc)
print("TEST PRECISION DEĞERİ(EN İYİ PARAMETRE İÇİN)", test_precision)
print("F1-SCORE SONUCU", f1)
m = 'Logistic Regression UYGULAMASI(DROP YAPILAN)'
acc.append([m, test_score, test_recall, test_recall,test_auc,test_precision,f1,fpr, tpr, thresholds])

best_score = 0 #SUPPORT VECTOR MACHINES(DESTEK VEKTÖR MAKİNELERİ) İÇİN NASIL DEĞERLER ALIYORUZ.KONTROL EDELİM

for c_paramter in [0.001, 0.01, 0.1, 1, 10, 100, 1000]: # parameter C
    for gamma_paramter in [0.001, 0.01, 0.1, 1, 10, 100, 1000]: # parameter gamma
        for k_parameter in ['rbf', 'linear', 'poly', 'sigmoid']: #  kernel parameter
            svmModel = SVC(kernel=k_parameter, C=c_paramter, gamma=gamma_paramter) # MODELİ OLUŞTURUYORUZ

            scores = cross_val_score(svmModel, X_trainval_scaled1, Y_trainval1, cv=kfolds, scoring='accuracy')
            # TRANING SET , TRAINING VE cross_validation İÇİN DAĞITILMIŞ OLACAK

            # cross_validation SONUCU ELDE ETTİĞİMİZ ACCURACY DEĞERLERİNİN ORTALAMASINI ALIYORUZ.
            score = np.mean(scores)

            if score > best_score:
                best_score = score
                best_parameter_c = c_paramter
                best_parameter_gamma = gamma_paramter
                best_parameter_k = k_parameter



SelectedSVMmodel = SVC(C=best_parameter_c, gamma=best_parameter_gamma, kernel=best_parameter_k).fit(X_trainval_scaled1, Y_trainval1)

test_score = SelectedSVMmodel.score(X_test_scaled1, Y_test1)
PredictedOutput = SelectedSVMmodel.predict(X_test_scaled1)
test_recall = recall_score(Y_test1, PredictedOutput, pos_label=1)
fpr, tpr, thresholds = roc_curve(Y_test1, PredictedOutput, pos_label=1)
test_auc = auc(fpr, tpr)
test_precision = precision_score(Y_test1, PredictedOutput, pos_label=1)
f1 = f1_score(Y_test1, PredictedOutput, pos_label=1)
print("cross_validation İÇİN EN İYİ ACCURACY DEĞERİ :", best_score)
print("c İÇİN EN İYİ PARAMETRE : ", best_parameter_c)
print("gamma İÇİN EN İYİ PARAMETRE : ", best_parameter_gamma)
print("kernel İÇİN EN İYİ PARAMETRE : ", best_parameter_k)
print("TEST ACCURACY DEĞERİ (EN İYİ PARAMETRELER İLE) ", test_score)
print("TEST RECALL DEĞERİ (EN İYİ PARAMETRELER İLE) ", test_recall)
print("TEST AUC DEĞERİ (EN İYİ PARAMETRELER İLE )", test_auc)
print("TEST PRECISION DEĞERİ(EN İYİ PARAMETRE İÇİN)", test_precision)
print("F1-SCORE SONUCU", f1)
m = 'SVM'
acc.append([m, test_score, test_recall, test_auc,test_precision,f1, fpr, tpr, thresholds])

best_score = 0 #DECISION TREES(KARAR AĞAÇLARI ) YÖNTEMİNİ KONTROL EDELİM

for md in range(1, 9): # MAKSİMUM depth DEĞERLERİ İÇİN İTERASYON YAPIYORUZ
    # MODELİ EĞİTİYORUZ
    treeModel = DecisionTreeClassifier(random_state=0, max_depth=md, criterion='gini')

    scores = cross_val_score(treeModel, X_trainval_scaled, Y_trainval, cv=kfolds, scoring='accuracy')


    score = np.mean(scores)


    if score > best_score:
        best_score = score
        best_parameter = md


SelectedDTModel = DecisionTreeClassifier(max_depth=best_parameter).fit(X_trainval_scaled, Y_trainval )

test_score = SelectedDTModel.score(X_test_scaled, Y_test)
PredictedOutput = SelectedDTModel.predict(X_test_scaled)
test_recall = recall_score(Y_test, PredictedOutput, pos_label=1)
fpr, tpr, thresholds = roc_curve(Y_test, PredictedOutput, pos_label=1)
test_auc = auc(fpr, tpr)
test_precision = precision_score(Y_test, PredictedOutput, pos_label=1)
f1 = f1_score(Y_test, PredictedOutput, pos_label=1)
print("VALİDASYON SETİ İÇİN EN İYİ ACCURACY DEĞERİ : ", best_score)
print("MAKSİMUM depth İÇİN EN UYGUN PARAMETRE DEĞERİ (c) : ", best_parameters)
print("TEST ACCURACY DEĞERİ(EN İYİ PARAMETRE İÇİN)", test_score)
print("TEST RECALL DEĞERİ (EN İYİ PARAMETRE İÇİN)", test_recall)
print("TEST AUC DEĞERİ (EN İYİ PARAMETRE DEĞERİ İÇİN)", test_auc)
print("TEST PRECISION DEĞERİ(EN İYİ PARAMETRE İÇİN)", test_precision)
print("F1-SCORE SONUCU", f1)
m = 'Decision Tree (Karar Ağacı)'
acc.append([m, test_score, test_recall, test_auc,test_precision,f1, fpr, tpr, thresholds])

print("Feature importance: ")
np.array([X.columns.values.tolist(), list(SelectedDTModel.feature_importances_)]).T

from sklearn.tree import export_graphviz
import graphviz
dot_data=export_graphviz(SelectedDTModel,out_file=None, feature_names = X_trainval.columns.values.tolist())
graph = graphviz.Source(dot_data)
graph

best_score = 0 # RANDOM FOREST (RASTGELE AĞAÇLAR) UYGULADIĞIMIZ ZAMAN MODELİMİZE BAKALIM.

for M in range(2, 15, 2): # M KOMBİNE AĞAC
    for d in range(1, 9): # HER BÖLMEDE(DAĞILIMDA) DİKKATE ALINAN MAKSİMUM ÖZELLİK SAYISI
        for m in range(1, 9): # AĞACIN MAKSiMUM DERİNLİĞİ
            # MODELİ EĞİTİYORUZ
            # n_jobs(4) : PARALEL HESAPLAMA SAYISI
            forestModel = RandomForestClassifier(n_estimators=M, max_features=d, n_jobs=4,
                                          max_depth=m, random_state=0)


            scores = cross_val_score(forestModel, X_trainval_scaled, Y_trainval, cv=kfolds, scoring='accuracy')

            # crossval İLE ACCURACY DEĞERLERİNİN ORTALAMALARI
            score = np.mean(scores)


            if score > best_score:
                best_score = score
                best_M = M
                best_d = d
                best_m = m

# MODELİ YENİDEN KURUYORUZ
SelectedRFModel = RandomForestClassifier(n_estimators=M, max_features=d,
                                          max_depth=m, random_state=0).fit(X_trainval_scaled, Y_trainval )

PredictedOutput = SelectedRFModel.predict(X_test_scaled)
test_score = SelectedRFModel.score(X_test_scaled, Y_test)
test_recall = recall_score(Y_test, PredictedOutput, pos_label=1)
fpr, tpr, thresholds = roc_curve(Y_test, PredictedOutput, pos_label=1)
test_auc = auc(fpr, tpr)
test_precision = precision_score(Y_test, PredictedOutput, pos_label=1)
f1 = f1_score(Y_test, PredictedOutput, pos_label=1)
print("VALİDASYON SETİ ÜZERİNDE EN İYİ ACCURACY:", best_score)
print("EN İYİ PARAMETRELER İÇİN M,d,m DEĞERLERİ: ", best_M, best_d, best_m)
print("TEST ACCURACY DEĞERİ", test_score)
print("TEST RECALL:", test_recall)
print("TEST ROC:", test_auc)
print("TEST PRECISION DEĞERİ(EN İYİ PARAMETRE İÇİN)", test_precision)
print("F1-SCORE SONUCU", f1)


m = 'Random Forest'
acc.append([m, test_score, test_recall, test_auc,test_precision,f1, fpr, tpr, thresholds])

print("Feature importance: ")
np.array([X.columns.values.tolist(), list(SelectedRFModel.feature_importances_)]).T

best_score = 0 #ADABOOST İÇİN YAPIYORUZ.

for M in range(2, 15, 2): # M AĞAÇLARINI KOMBİNE ET
    for lr in [0.0001, 0.001, 0.01, 0.1, 1]:

        boostModel = AdaBoostClassifier(n_estimators=M, learning_rate=lr, random_state=0)


        scores = cross_val_score(boostModel, X_trainval_scaled, Y_trainval, cv=kfolds, scoring='accuracy')


        score = np.mean(scores)


        if score > best_score:
            best_score = score
            best_M = M
            best_lr = lr

# TRAINING VE TEST SETLERİNİ KULLANARAK MODELİMİZ GÜNCELLİYORUZ.
SelectedBoostModel = AdaBoostClassifier(n_estimators=M, learning_rate=lr, random_state=0).fit(X_trainval_scaled, Y_trainval )

PredictedOutput = SelectedBoostModel.predict(X_test_scaled)
test_score = SelectedRFModel.score(X_test_scaled, Y_test)
test_recall = recall_score(Y_test, PredictedOutput, pos_label=1)
fpr, tpr, thresholds = roc_curve(Y_test, PredictedOutput, pos_label=1)
test_auc = auc(fpr, tpr)
test_precision = precision_score(Y_test, PredictedOutput, pos_label=1)
f1 = f1_score(Y_test, PredictedOutput, pos_label=1)
print("VALİDASYON SETİ İÇİN EN İYİ ACCURACY :", best_score)
print("M İÇİN EN PARAMETRE : ", best_M)
print("LEARNING_RATE(LR) İÇİN EN İYİ PARAMETRE : ", best_lr)
print("TEST ACCURACY DEĞERİ(EN İYİ PARAMETRELER İLE) ", test_score)
print("TEST RECALL DEĞERİ(EN İYİ PARAMETRELER İLE) ", test_recall)
print("TEST AUC DEĞERİ(EN İYİ PARAMETRELER İLE)", test_auc)
print("TEST PRECISION DEĞERİ(EN İYİ PARAMETRE İÇİN)", test_precision)
print("F1-SCORE SONUCU", f1)


m = 'AdaBoost'
acc.append([m, test_score, test_recall, test_auc,test_precision,f1, fpr, tpr, thresholds])

print("Feature importance: ")
np.array([X.columns.values.tolist(), list(SelectedBoostModel.feature_importances_)]).T

# BİLGİNİN EKLEDİĞİ (BATCH SIZE VE EPOCH SAYISINI ÖN PLANA ÇIKARAN)
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.wrappers.scikit_learn import KerasClassifier
from keras.constraints import maxnorm
from numpy import genfromtxt


# KerasClassifier için gerekli modelin oluşturulması
def create_model(dropout_rate=0.0, weight_constraint=0):
	#KERAS ardışık modelin başlatılması
	model = Sequential()
	model.add(Dense(12, input_dim=15, kernel_initializer='uniform', activation='relu', kernel_constraint=maxnorm(weight_constraint)))
	model.add(Dropout(dropout_rate))
	model.add(Dense(1, kernel_initializer='uniform', activation='softmax'))
	# Modeli derleyelim
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model


seed = 7
numpy.random.seed(seed)
# Veri setinin yüklenmesi
dataset = genfromtxt("oasis_longitudinal.csv", delimiter=",")
# Dilimlemeyi kullanarak giriş değerlerini X ve etiket değerlerine Y yükleme
#X = dataset[:,0:30]
#Y = dataset[:,1]
# Model oluşturma
model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=30, verbose=0)
# define the grid search parameters
weight_constraint = [1, 2]
dropout_rate = [0.0, 0.1]
param_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)
grid = GridSearchCV(estimator=model, param_grid=param_grid)
grid_result = grid.fit(X, Y)
# Sonuçlar
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

# OPTIMIZASYON ALGORİTMALARI ÜZERİNDE DEĞİŞİKLİK YAPARAK
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
# Function to create model, required for KerasClassifier
def create_model(optimizer='adam'):
	# create model
	model = Sequential()
	model.add(Dense(12, input_dim=8, activation='relu'))
	model.add(Dense(1, activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
	return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataset = numpy.genfromtxt("oasis_longitudinal.csv", delimiter=",")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = KerasClassifier(build_fn=create_model, epochs=40, batch_size=10, verbose=0)
# define the grid search parameters
optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']
param_grid = dict(optimizer=optimizer)
grid = GridSearchCV(estimator=model, param_grid=param_grid)
grid_result = grid.fit(X, Y)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

# LEARNING RATE VE MOMENTUM DEĞİŞEREK YAPILAN SİNİR AĞI
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from keras.optimizers import SGD
# Function to create model, required for KerasClassifier
def create_model(learn_rate=0.01, momentum=0):
	# create model
	model = Sequential()
	model.add(Dense(12, input_dim=8, activation='relu'))
	model.add(Dense(1, activation='sigmoid'))
	# Compile model
	optimizer = SGD(lr=learn_rate, momentum=momentum)
	model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
	return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataset = numpy.genfromtxt("oasis_longitudinal.csv", delimiter=",")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)
# define the grid search parameters
learn_rate = [0.001, 0.01, 0.1]
momentum = [0.0, 0.2, 0.4]
param_grid = dict(learn_rate=learn_rate, momentum=momentum)
grid = GridSearchCV(estimator=model, param_grid=param_grid )
grid_result = grid.fit(X, Y)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

# AĞIRLIKLARI UYGUN ŞEKİLDE BAŞLATMA İLE
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
# Function to create model, required for KerasClassifier
def create_model(init_mode='uniform'):
	# create model
	model = Sequential()
	model.add(Dense(12, input_dim=8, kernel_initializer=init_mode, activation='relu'))
	model.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataset = numpy.genfromtxt("oasis_longitudinal.csv", delimiter=",")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)
# define the grid search parameters
init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']
param_grid = dict(init_mode=init_mode)
grid = GridSearchCV(estimator=model, param_grid=param_grid)
grid_result = grid.fit(X, Y)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

# AKTİVASYON FONKSYONLARI ÜZERİNDE İŞLEM YAPARAK DENEYELİM
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
# Function to create model, required for KerasClassifier
def create_model(activation='relu'):
	# create model
	model = Sequential()
	model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation=activation))
	model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)
# define the grid search parameters
activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']
param_grid = dict(activation=activation)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(X, Y)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))# Use scikit-learn to grid search the activation function
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
# Function to create model, required for KerasClassifier
def create_model(activation='relu'):
	# create model
	model = Sequential()
	model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation=activation))
	model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)
# define the grid search parameters
activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']
param_grid = dict(activation=activation)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(X, Y)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

# DROPOUT İŞLEMİ DÜZENLENEREK (BİLGİNİN AYNISI)
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.wrappers.scikit_learn import KerasClassifier
from keras.constraints import maxnorm
# Function to create model, required for KerasClassifier
def create_model(dropout_rate=0.0, weight_constraint=0):
	# create model
	model = Sequential()
	model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu', kernel_constraint=maxnorm(weight_constraint)))
	model.add(Dropout(dropout_rate))
	model.add(Dense(1, kernel_initializer='uniform', activation='softmax'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataset = numpy.genfromtxt("oasis_longitudinal.csv", delimiter=",")
# split into input (X) and output (Y) variables
X = dataset[:,0:30]
Y = dataset[:,10]
# create model
model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)
# define the grid search parameters
weight_constraint = [1, 2, 3, 4, 5]
dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4]
param_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)
grid = GridSearchCV(estimator=model, param_grid=param_grid)
grid_result = grid.fit(X_trainval_scaled, Y_trainval)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

#GİZLİ KATMAN SAYISI DEĞİŞTİRİLEREK YAPILAN SİNİR AĞI
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.wrappers.scikit_learn import KerasClassifier
from keras.constraints import maxnorm
# Function to create model, required for KerasClassifier
def create_model(neurons=1):
	# create model
	model = Sequential()
	model.add(Dense(neurons, input_dim=8, kernel_initializer='uniform', activation='linear', kernel_constraint=maxnorm(4)))
	model.add(Dropout(0.2))
	model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataset = numpy.genfromtxt("oasis_longitudinal.csv", delimiter=",")
# split into input (X) and output (Y) variables
#X = dataset[:,0:8]
#Y = dataset[:,8]
# create model
model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)
# define the grid search parameters
neurons = [1, 5, 10]
param_grid = dict(neurons=neurons)
grid = GridSearchCV(estimator=model, param_grid=param_grid,)
grid_result = grid.fit(X_trainval_scaled, Y_trainval)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

#HEPSİ BURADA
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.wrappers.scikit_learn import KerasClassifier
from keras.constraints import maxnorm


def create_model(neurons=1,dropout_rate=0.0, weight_constraint=0,activation='relu',init_mode='uniform',optimizer='adam'):

	model = Sequential()
	model.add(Dense(neurons, input_dim=8, kernel_initializer=init_mode, activation=activation, kernel_constraint=maxnorm(weight_constraint)))
	model.add(Dropout(dropout_rate))
	model.add(Dense(1, kernel_initializer=init_mode, activation='relu'))


	model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
	return model

seed = 7
numpy.random.seed(seed)

dataset = numpy.genfromtxt("oasis_longitudinal.csv", delimiter=",")

X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)

neurons = [1, 5, 10]
weight_constraint = [1, 2, 3, 4]
dropout_rate = [0.0, 0.1, 0.5]
activation = ['softmax', 'softplus', 'softsign', 'relu','sigmoid']
init_mode = ['uniform', 'lecun_uniform', 'normal']
optimizer = ['SGD', 'RMSprop', 'Adagrad','Adam']

param_grid = dict(neurons=neurons,dropout_rate=dropout_rate, weight_constraint=weight_constraint,activation=activation,optimizer=optimizer)
grid = GridSearchCV(estimator=model, param_grid=param_grid)
grid_result = grid.fit(X_trainval_scaled, Y_trainval)

print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

# METRİKLERİN PERFORMANSI
result = pd.DataFrame(acc, columns=['Model', 'Accuracy', 'Recall', 'AUC','Precision','F1', 'FPR', 'TPR', 'TH','temp']) # UĞRAŞTIĞIM HATAYI 'temp' YAZARAK GİDERDİM
result[['Model', 'Accuracy', 'Recall', 'AUC','Precision','F1']]

"""**1 ) EN SON GÖRÜNTÜ PENCERESİNE F SCORE RECALL PRECISION PARAMETRELERİN HEPSİNİ EKLE (BİLGİ BULDU) (BİLGİ BUNLARIN HEPSİNİ EKLEYECEK)

2 ) csv DOSYASI İŞLEMEKTE EN UYGUN YSA NEDİR ONA BAK(SALİH PCA DAN SONRA BAKACAK)

3 ) LİTERATÜRDEKİLERİN NASIL YAPTIKLARINI BAK (YAPTIKLARININ ÜZERİNE batch_normalization İŞLEMİNİ YAP)(DERİN ÖĞRENE UYGULARKEN YAPACAĞIZ)

4 ) PCA YI UYGULAYACAĞIZ NORMALDE OLANLARI(PCA TAMAM cross iLE OLANINI BUL)(PCA UYGULAMASINI SALİH BULACAK)


7 ) HEPSİNİ TEK TEK AÇIKLAMA SATIRI YAZ ANLAŞILMIYOR.(PCADAN SONRA BAKILACAK)


"""